{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:54:07.828723Z","iopub.status.busy":"2023-05-22T06:54:07.828374Z","iopub.status.idle":"2023-05-22T06:54:17.769659Z","shell.execute_reply":"2023-05-22T06:54:17.768730Z","shell.execute_reply.started":"2023-05-22T06:54:07.828694Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","from glob import glob\n","from ast import literal_eval\n","import jsonlines\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","# from sklearn.metrics import plot_confusion_matrix\n","import random\n","from sklearn.utils import shuffle\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from keras.preprocessing.text import Tokenizer\n","seed = 2022"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:04:02.424561Z","iopub.status.busy":"2023-05-22T07:04:02.420578Z","iopub.status.idle":"2023-05-22T07:07:39.474056Z","shell.execute_reply":"2023-05-22T07:07:39.473036Z","shell.execute_reply.started":"2023-05-22T07:04:02.424512Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Read 1M words\n","Number of words:  6162\n","Number of labels: 0\n","Progress: 100.0% words/sec/thread:   45147 lr:  0.000000 avg.loss:  2.092068 ETA:   0h 0m 0s\n","/tmp/ipykernel_32/2386768707.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n","  weights = torch.FloatTensor(vectors)\n"]}],"source":["import fasttext\n","\n","# train word vector\n","model = fasttext.train_unsupervised('/kaggle/input/allprivacy/all.txt', model='skipgram', dim=300)\n","\n","# 获取词汇表中的单词列表\n","words = model.words\n","\n","# 获取每个单词的词向量\n","vectors = []\n","for word in words:\n","    vector = model[word]\n","    vectors.append(vector)\n","\n","# 将词向量转换为PyTorch张量\n","weights = torch.FloatTensor(vectors)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T02:13:14.442789Z","iopub.status.busy":"2023-05-12T02:13:14.442312Z","iopub.status.idle":"2023-05-12T02:13:16.887767Z","shell.execute_reply":"2023-05-12T02:13:16.886404Z","shell.execute_reply.started":"2023-05-12T02:13:14.442744Z"},"trusted":true},"outputs":[],"source":["model.save_model('/kaggle/working/fasttext')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T08:19:28.744913Z","iopub.status.busy":"2023-05-12T08:19:28.744141Z","iopub.status.idle":"2023-05-12T08:19:28.773142Z","shell.execute_reply":"2023-05-12T08:19:28.771712Z","shell.execute_reply.started":"2023-05-12T08:19:28.744843Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([6162, 300])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["weights.shape"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T08:43:48.139841Z","iopub.status.busy":"2023-05-12T08:43:48.137150Z","iopub.status.idle":"2023-05-12T08:43:48.148399Z","shell.execute_reply":"2023-05-12T08:43:48.146783Z","shell.execute_reply.started":"2023-05-12T08:43:48.139766Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([6162, 100])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["weights"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T06:54:31.013269Z","iopub.status.busy":"2023-05-22T06:54:31.011953Z","iopub.status.idle":"2023-05-22T06:54:31.161469Z","shell.execute_reply":"2023-05-22T06:54:31.160402Z","shell.execute_reply.started":"2023-05-22T06:54:31.013225Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/opp-multilabel-9class/allclass/trainAll.csv')\n","valid = pd.read_csv('/kaggle/input/opp-multilabel-9class/allclass/validationALL.csv')\n","test = pd.read_csv('/kaggle/input/opp-multilabel-9class/allclass/testALL.csv')\n","\n","def str2int(df):\n","    for col in df.columns[1:]:\n","#         print(col)\n","        df[col] = df[col].apply(lambda x: 1 if x == col else 0)\n","    return df\n","\n","train = str2int(train)\n","valid = str2int(valid)\n","test = str2int(test)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T05:54:19.234341Z","iopub.status.busy":"2023-05-12T05:54:19.233856Z","iopub.status.idle":"2023-05-12T05:54:34.605532Z","shell.execute_reply":"2023-05-12T05:54:34.604090Z","shell.execute_reply.started":"2023-05-12T05:54:19.234299Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Read 0M words\n","Number of words:  3607\n","Number of labels: 0\n","Progress: 100.0% words/sec/thread:   34001 lr:  0.000000 avg.loss:  2.412618 ETA:   0h 0m 0s\n"]}],"source":["test_model = fasttext.train_unsupervised('/kaggle/input/4fasttext-vec/opptext.txt', model='skipgram', dim=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["words = model.words\n","words"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:10:43.907641Z","iopub.status.busy":"2023-05-22T07:10:43.907286Z","iopub.status.idle":"2023-05-22T07:10:43.935247Z","shell.execute_reply":"2023-05-22T07:10:43.934094Z","shell.execute_reply.started":"2023-05-22T07:10:43.907612Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:10:52.686535Z","iopub.status.busy":"2023-05-22T07:10:52.686188Z","iopub.status.idle":"2023-05-22T07:10:52.706115Z","shell.execute_reply":"2023-05-22T07:10:52.704872Z","shell.execute_reply.started":"2023-05-22T07:10:52.686509Z"},"trusted":true},"outputs":[],"source":["\n","#     def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dense_size):\n","#         super(MultiLabelCNN, self).__init__()\n","#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","#         self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (f, embedding_dim)) for f in filter_sizes])\n","# #         self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n","#         self.fc1 = nn.Linear(len(filter_sizes) * num_filters, dense_size)\n","#         self.fc2 = nn.Linear(dense_size, num_classes)\n","        \n","\n","#     def forward(self, x):\n","#         x = self.embedding(x) # [batch_size, seq_len, embedding_dim]\n","#         x = x.unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n","#         x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # [batch_size, num_filters, seq_len] * len(filter_sizes)\n","#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # [batch_size, num_filters] * len(filter_sizes)\n","#         x = torch.cat(x, 1) # [batch_size, num_filters * len(filter_sizes)]\n","#         logits = self.fc(x) # [batch_size, num_classes]\n","#         output = torch.sigmoid(logits)\n","#         return output\n","class MultiLabelCNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dense_size):\n","        super(MultiLabelCNN, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx = 0)\n","#         self.embedding = nn.Embedding.from_pretrained(embeddings=pretrained_weights,freeze=True)\n","        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (f, embedding_dim)) for f in filter_sizes])\n","        self.relu = nn.ReLU()\n","        self.drop_out = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, dense_size)\n","        self.fc2 = nn.Linear(dense_size, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.embedding(x) # [batch_size, seq_len, embedding_dim]\n","        x = x.unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # [batch_size, num_filters, seq_len] * len(filter_sizes)\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # [batch_size, num_filters] * len(filter_sizes)\n","        x = torch.cat(x, 1) # [batch_size, num_filters * len(filter_sizes)]\n","        x = F.relu(self.fc1(x)) # [batch_size, dense_size]\n","        x = self.drop_out(x)\n","        logits = self.fc2(x) # [batch_size, num_classes]\n","        \n","        output = torch.sigmoid(logits)\n","        return output\n","    def load_pretrained_embeddings(self, weights_matrix):\n","        self.embedding = self.embedding.from_pretrained(weights_matrix,freeze = True)\n","            \n","    def fit(self, x_train, y_train, x_val, y_val, batch_size=40, num_epochs=10, learning_rate=0.1):\n","        x_train = x_train.to(device)\n","        y_train = y_train.to(device)\n","        x_val = x_val.to(device)\n","        y_val = y_val.to(device)\n","        criterion = nn.BCELoss()\n","        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n","        for epoch in range(num_epochs):\n","            model.train()\n","            permutation = torch.randperm(x_train.size()[0]).to(device)\n","            for i in range(0,x_train.size()[0], batch_size):\n","                indices = permutation[i:i+batch_size]\n","                batch_x, batch_y = x_train[indices], y_train[indices]\n","                y_pred = self(batch_x)\n","                loss = criterion(y_pred, batch_y)\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","            model.eval()\n","            val_loss = criterion(self(x_val), y_val)\n","            print(f'Epoch {epoch + 1}/{num_epochs}: Loss = {loss.item():.4f}, Val Loss = {val_loss.item():.4f}')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:10:57.773423Z","iopub.status.busy":"2023-05-22T07:10:57.773006Z","iopub.status.idle":"2023-05-22T07:11:00.329194Z","shell.execute_reply":"2023-05-22T07:11:00.328194Z","shell.execute_reply.started":"2023-05-22T07:10:57.773390Z"},"trusted":true},"outputs":[],"source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","\n","train_text = train['text'].values\n","train_tokens = [tokenizer.tokenize(t) for t in train_text]\n","\n","valid_text = valid['text'].values\n","valid_tokens = [tokenizer.tokenize(t) for t in valid_text]\n","\n","test_text = test['text'].values\n","test_tokens = [tokenizer.tokenize(t) for t in test_text]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:11:00.686607Z","iopub.status.busy":"2023-05-22T07:11:00.686239Z","iopub.status.idle":"2023-05-22T07:11:00.693025Z","shell.execute_reply":"2023-05-22T07:11:00.691987Z","shell.execute_reply.started":"2023-05-22T07:11:00.686576Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["whole_tokens = train_tokens + valid_tokens + test_tokens"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:11:01.144616Z","iopub.status.busy":"2023-05-22T07:11:01.144259Z","iopub.status.idle":"2023-05-22T07:11:01.150980Z","shell.execute_reply":"2023-05-22T07:11:01.150000Z","shell.execute_reply.started":"2023-05-22T07:11:01.144589Z"},"trusted":true},"outputs":[],"source":["# 构建词汇表\n","word2index = {word: index for index, word in enumerate(words)}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:18:48.877921Z","iopub.status.busy":"2023-05-22T07:18:48.877552Z","iopub.status.idle":"2023-05-22T07:18:49.146333Z","shell.execute_reply":"2023-05-22T07:18:49.145359Z","shell.execute_reply.started":"2023-05-22T07:18:48.877877Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#fast-text vector\n","X_train = [[word2index.get(token.lower()) for token in tokens if token.lower() in word2index] for tokens in train_tokens]\n","X_train = [torch.tensor(tokens) for tokens in X_train]\n","X_train = pad_sequence(X_train, batch_first=True, padding_value=0)\n","\n","X_valid = [[word2index.get(token.lower()) for token in tokens if token.lower() in word2index] for tokens in valid_tokens]\n","X_valid = [torch.tensor(tokens) for tokens in X_valid]\n","X_valid = pad_sequence(X_valid, batch_first=True, padding_value=0)\n","\n","X_test = [[word2index.get(token.lower()) for token in tokens if token.lower() in word2index] for tokens in test_tokens]\n","X_test = [torch.tensor(tokens) for tokens in X_test]\n","X_test = pad_sequence(X_test, batch_first=True, padding_value=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T07:15:43.590920Z","iopub.status.busy":"2023-05-17T07:15:43.590502Z","iopub.status.idle":"2023-05-17T07:15:44.166341Z","shell.execute_reply":"2023-05-17T07:15:44.165139Z","shell.execute_reply.started":"2023-05-17T07:15:43.590891Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#用自带的word2vec\n","# 构建词汇表\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(whole_tokens)\n","\n","# 将tokens转换为整数索引\n","X_train = [torch.tensor(tokenizer.texts_to_sequences([t])[0]) for t in train_tokens]\n","\n","# 对齐序列长度\n","X_train = pad_sequence(X_train, batch_first=True, padding_value=0)\n","\n","X_valid = [torch.tensor(tokenizer.texts_to_sequences([t])[0]) for t in valid_tokens]\n","X_valid = pad_sequence(X_valid, batch_first=True, padding_value=0)\n","\n","X_test = [torch.tensor(tokenizer.texts_to_sequences([t])[0]) for t in test_tokens]\n","X_test = pad_sequence(X_test, batch_first=True, padding_value=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T08:36:54.569135Z","iopub.status.busy":"2023-05-15T08:36:54.568319Z","iopub.status.idle":"2023-05-15T08:36:54.613744Z","shell.execute_reply":"2023-05-15T08:36:54.612233Z","shell.execute_reply.started":"2023-05-15T08:36:54.569090Z"},"trusted":true},"outputs":[],"source":["X_test = [torch.tensor(tokenizer.texts_to_sequences([t])[0]) for t in test_tokens]\n","X_test = pad_sequence(X_test, batch_first=True, padding_value=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T06:26:01.968696Z","iopub.status.busy":"2023-05-15T06:26:01.968198Z","iopub.status.idle":"2023-05-15T06:26:02.010872Z","shell.execute_reply":"2023-05-15T06:26:02.009523Z","shell.execute_reply.started":"2023-05-15T06:26:01.968654Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[  31,   48, 1505,  ...,    0,    0,    0],\n","        [   5,   16, 1505,  ...,    0,    0,    0],\n","        [ 446,   32,    6,  ...,    0,    0,    0],\n","        ...,\n","        [ 122,  629,    3,  ...,    0,    0,    0],\n","        [1000,    9,   34,  ...,    0,    0,    0],\n","        [  36,  399,    7,  ...,    0,    0,    0]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:19:14.298112Z","iopub.status.busy":"2023-05-22T07:19:14.297743Z","iopub.status.idle":"2023-05-22T07:19:14.307466Z","shell.execute_reply":"2023-05-22T07:19:14.306433Z","shell.execute_reply.started":"2023-05-22T07:19:14.298082Z"},"trusted":true},"outputs":[],"source":["vocab_size = len(words) + 1\n","# vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 300\n","num_filters = 200\n","filter_sizes = [3, 3, 3]\n","dense_size = 100\n","batch_size = 40\n","num_epochs = 200"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:19:18.164844Z","iopub.status.busy":"2023-05-22T07:19:18.164482Z","iopub.status.idle":"2023-05-22T07:19:18.179941Z","shell.execute_reply":"2023-05-22T07:19:18.176817Z","shell.execute_reply.started":"2023-05-22T07:19:18.164812Z"},"trusted":true},"outputs":[],"source":["# 加载标签数据\n","y_train = train[[x for x in train.columns[1:]]].values\n","num_classes = y_train.shape[1]\n","y_valid = valid[[x for x in valid.columns[1:]]].values"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:19:26.976716Z","iopub.status.busy":"2023-05-22T07:19:26.976014Z","iopub.status.idle":"2023-05-22T07:19:26.989953Z","shell.execute_reply":"2023-05-22T07:19:26.988836Z","shell.execute_reply.started":"2023-05-22T07:19:26.976681Z"},"trusted":true},"outputs":[],"source":["y_train = torch.tensor(y_train).float()\n","y_valid = torch.tensor(y_valid).float()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:20:20.304987Z","iopub.status.busy":"2023-05-22T07:20:20.304598Z","iopub.status.idle":"2023-05-22T07:22:09.067088Z","shell.execute_reply":"2023-05-22T07:22:09.066175Z","shell.execute_reply.started":"2023-05-22T07:20:20.304954Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MultiLabelCNN(\n","  (embedding): Embedding(6162, 300)\n","  (convs): ModuleList(\n","    (0-2): 3 x Conv2d(1, 200, kernel_size=(3, 300), stride=(1, 1))\n","  )\n","  (relu): ReLU()\n","  (drop_out): Dropout(p=0.5, inplace=False)\n","  (fc1): Linear(in_features=600, out_features=100, bias=True)\n","  (fc2): Linear(in_features=100, out_features=9, bias=True)\n",")\n","Epoch 1/200: Loss = 0.3775, Val Loss = 0.3699\n","Epoch 2/200: Loss = 0.3501, Val Loss = 0.3656\n","Epoch 3/200: Loss = 0.3619, Val Loss = 0.3643\n","Epoch 4/200: Loss = 0.3794, Val Loss = 0.3639\n","Epoch 5/200: Loss = 0.3960, Val Loss = 0.3634\n","Epoch 6/200: Loss = 0.3265, Val Loss = 0.3625\n","Epoch 7/200: Loss = 0.3694, Val Loss = 0.3619\n","Epoch 8/200: Loss = 0.3291, Val Loss = 0.3609\n","Epoch 9/200: Loss = 0.3907, Val Loss = 0.3605\n","Epoch 10/200: Loss = 0.3712, Val Loss = 0.3601\n","Epoch 11/200: Loss = 0.3191, Val Loss = 0.3589\n","Epoch 12/200: Loss = 0.3344, Val Loss = 0.3581\n","Epoch 13/200: Loss = 0.3405, Val Loss = 0.3574\n","Epoch 14/200: Loss = 0.3505, Val Loss = 0.3562\n","Epoch 15/200: Loss = 0.3757, Val Loss = 0.3546\n","Epoch 16/200: Loss = 0.3549, Val Loss = 0.3530\n","Epoch 17/200: Loss = 0.3368, Val Loss = 0.3511\n","Epoch 18/200: Loss = 0.3596, Val Loss = 0.3490\n","Epoch 19/200: Loss = 0.3554, Val Loss = 0.3459\n","Epoch 20/200: Loss = 0.3495, Val Loss = 0.3424\n","Epoch 21/200: Loss = 0.3592, Val Loss = 0.3380\n","Epoch 22/200: Loss = 0.3112, Val Loss = 0.3339\n","Epoch 23/200: Loss = 0.3543, Val Loss = 0.3285\n","Epoch 24/200: Loss = 0.3633, Val Loss = 0.3233\n","Epoch 25/200: Loss = 0.3006, Val Loss = 0.3177\n","Epoch 26/200: Loss = 0.3450, Val Loss = 0.3123\n","Epoch 27/200: Loss = 0.3151, Val Loss = 0.3058\n","Epoch 28/200: Loss = 0.2637, Val Loss = 0.2998\n","Epoch 29/200: Loss = 0.3118, Val Loss = 0.2937\n","Epoch 30/200: Loss = 0.3055, Val Loss = 0.2877\n","Epoch 31/200: Loss = 0.2892, Val Loss = 0.2826\n","Epoch 32/200: Loss = 0.2302, Val Loss = 0.2773\n","Epoch 33/200: Loss = 0.2744, Val Loss = 0.2725\n","Epoch 34/200: Loss = 0.2590, Val Loss = 0.2672\n","Epoch 35/200: Loss = 0.2468, Val Loss = 0.2626\n","Epoch 36/200: Loss = 0.2408, Val Loss = 0.2583\n","Epoch 37/200: Loss = 0.2646, Val Loss = 0.2542\n","Epoch 38/200: Loss = 0.2676, Val Loss = 0.2517\n","Epoch 39/200: Loss = 0.2145, Val Loss = 0.2476\n","Epoch 40/200: Loss = 0.2086, Val Loss = 0.2433\n","Epoch 41/200: Loss = 0.2934, Val Loss = 0.2393\n","Epoch 42/200: Loss = 0.1869, Val Loss = 0.2363\n","Epoch 43/200: Loss = 0.1880, Val Loss = 0.2343\n","Epoch 44/200: Loss = 0.1940, Val Loss = 0.2311\n","Epoch 45/200: Loss = 0.2808, Val Loss = 0.2263\n","Epoch 46/200: Loss = 0.2596, Val Loss = 0.2239\n","Epoch 47/200: Loss = 0.2734, Val Loss = 0.2205\n","Epoch 48/200: Loss = 0.2056, Val Loss = 0.2200\n","Epoch 49/200: Loss = 0.2542, Val Loss = 0.2160\n","Epoch 50/200: Loss = 0.1693, Val Loss = 0.2129\n","Epoch 51/200: Loss = 0.1805, Val Loss = 0.2121\n","Epoch 52/200: Loss = 0.1953, Val Loss = 0.2079\n","Epoch 53/200: Loss = 0.1763, Val Loss = 0.2062\n","Epoch 54/200: Loss = 0.1714, Val Loss = 0.2039\n","Epoch 55/200: Loss = 0.1593, Val Loss = 0.2021\n","Epoch 56/200: Loss = 0.2465, Val Loss = 0.2026\n","Epoch 57/200: Loss = 0.2191, Val Loss = 0.1996\n","Epoch 58/200: Loss = 0.1629, Val Loss = 0.1985\n","Epoch 59/200: Loss = 0.1714, Val Loss = 0.1966\n","Epoch 60/200: Loss = 0.1633, Val Loss = 0.1963\n","Epoch 61/200: Loss = 0.1742, Val Loss = 0.1951\n","Epoch 62/200: Loss = 0.2067, Val Loss = 0.1939\n","Epoch 63/200: Loss = 0.1920, Val Loss = 0.1924\n","Epoch 64/200: Loss = 0.1685, Val Loss = 0.1909\n","Epoch 65/200: Loss = 0.1314, Val Loss = 0.1912\n","Epoch 66/200: Loss = 0.1852, Val Loss = 0.1898\n","Epoch 67/200: Loss = 0.1547, Val Loss = 0.1894\n","Epoch 68/200: Loss = 0.1610, Val Loss = 0.1886\n","Epoch 69/200: Loss = 0.1264, Val Loss = 0.1881\n","Epoch 70/200: Loss = 0.1719, Val Loss = 0.1880\n","Epoch 71/200: Loss = 0.1656, Val Loss = 0.1867\n","Epoch 72/200: Loss = 0.1608, Val Loss = 0.1869\n","Epoch 73/200: Loss = 0.1297, Val Loss = 0.1871\n","Epoch 74/200: Loss = 0.1690, Val Loss = 0.1856\n","Epoch 75/200: Loss = 0.1661, Val Loss = 0.1860\n","Epoch 76/200: Loss = 0.1572, Val Loss = 0.1853\n","Epoch 77/200: Loss = 0.1309, Val Loss = 0.1864\n","Epoch 78/200: Loss = 0.1355, Val Loss = 0.1854\n","Epoch 79/200: Loss = 0.1922, Val Loss = 0.1876\n","Epoch 80/200: Loss = 0.1147, Val Loss = 0.1834\n","Epoch 81/200: Loss = 0.1187, Val Loss = 0.1851\n","Epoch 82/200: Loss = 0.0929, Val Loss = 0.1840\n","Epoch 83/200: Loss = 0.1319, Val Loss = 0.1875\n","Epoch 84/200: Loss = 0.1607, Val Loss = 0.1825\n","Epoch 85/200: Loss = 0.1297, Val Loss = 0.1842\n","Epoch 86/200: Loss = 0.1065, Val Loss = 0.1822\n","Epoch 87/200: Loss = 0.1269, Val Loss = 0.1827\n","Epoch 88/200: Loss = 0.0984, Val Loss = 0.1856\n","Epoch 89/200: Loss = 0.1496, Val Loss = 0.1846\n","Epoch 90/200: Loss = 0.1084, Val Loss = 0.1823\n","Epoch 91/200: Loss = 0.1678, Val Loss = 0.1831\n","Epoch 92/200: Loss = 0.1045, Val Loss = 0.1823\n","Epoch 93/200: Loss = 0.1261, Val Loss = 0.1828\n","Epoch 94/200: Loss = 0.1254, Val Loss = 0.1834\n","Epoch 95/200: Loss = 0.0748, Val Loss = 0.1820\n","Epoch 96/200: Loss = 0.1617, Val Loss = 0.1826\n","Epoch 97/200: Loss = 0.0783, Val Loss = 0.1819\n","Epoch 98/200: Loss = 0.1202, Val Loss = 0.1820\n","Epoch 99/200: Loss = 0.0985, Val Loss = 0.1828\n","Epoch 100/200: Loss = 0.0878, Val Loss = 0.1861\n","Epoch 101/200: Loss = 0.1116, Val Loss = 0.1825\n","Epoch 102/200: Loss = 0.0922, Val Loss = 0.1841\n","Epoch 103/200: Loss = 0.0740, Val Loss = 0.1825\n","Epoch 104/200: Loss = 0.1023, Val Loss = 0.1827\n","Epoch 105/200: Loss = 0.1606, Val Loss = 0.1810\n","Epoch 106/200: Loss = 0.0719, Val Loss = 0.1846\n","Epoch 107/200: Loss = 0.0956, Val Loss = 0.1820\n","Epoch 108/200: Loss = 0.0953, Val Loss = 0.1836\n","Epoch 109/200: Loss = 0.1037, Val Loss = 0.1846\n","Epoch 110/200: Loss = 0.1059, Val Loss = 0.1828\n","Epoch 111/200: Loss = 0.1367, Val Loss = 0.1838\n","Epoch 112/200: Loss = 0.0800, Val Loss = 0.1860\n","Epoch 113/200: Loss = 0.0752, Val Loss = 0.1859\n","Epoch 114/200: Loss = 0.0650, Val Loss = 0.1861\n","Epoch 115/200: Loss = 0.0545, Val Loss = 0.1838\n","Epoch 116/200: Loss = 0.0908, Val Loss = 0.1850\n","Epoch 117/200: Loss = 0.1040, Val Loss = 0.1867\n","Epoch 118/200: Loss = 0.0582, Val Loss = 0.1861\n","Epoch 119/200: Loss = 0.0915, Val Loss = 0.1918\n","Epoch 120/200: Loss = 0.0983, Val Loss = 0.1846\n","Epoch 121/200: Loss = 0.0729, Val Loss = 0.1890\n","Epoch 122/200: Loss = 0.0579, Val Loss = 0.1886\n","Epoch 123/200: Loss = 0.1060, Val Loss = 0.1877\n","Epoch 124/200: Loss = 0.0410, Val Loss = 0.1877\n","Epoch 125/200: Loss = 0.0475, Val Loss = 0.1902\n","Epoch 126/200: Loss = 0.0887, Val Loss = 0.1876\n","Epoch 127/200: Loss = 0.0660, Val Loss = 0.1876\n","Epoch 128/200: Loss = 0.0698, Val Loss = 0.1900\n","Epoch 129/200: Loss = 0.0581, Val Loss = 0.1895\n","Epoch 130/200: Loss = 0.0837, Val Loss = 0.1895\n","Epoch 131/200: Loss = 0.0643, Val Loss = 0.1900\n","Epoch 132/200: Loss = 0.0478, Val Loss = 0.1913\n","Epoch 133/200: Loss = 0.0461, Val Loss = 0.1922\n","Epoch 134/200: Loss = 0.0573, Val Loss = 0.1901\n","Epoch 135/200: Loss = 0.0448, Val Loss = 0.1938\n","Epoch 136/200: Loss = 0.0745, Val Loss = 0.1955\n","Epoch 137/200: Loss = 0.0693, Val Loss = 0.1943\n","Epoch 138/200: Loss = 0.0823, Val Loss = 0.1933\n","Epoch 139/200: Loss = 0.0680, Val Loss = 0.1931\n","Epoch 140/200: Loss = 0.0952, Val Loss = 0.1934\n","Epoch 141/200: Loss = 0.0663, Val Loss = 0.1922\n","Epoch 142/200: Loss = 0.0620, Val Loss = 0.1902\n","Epoch 143/200: Loss = 0.0655, Val Loss = 0.1954\n","Epoch 144/200: Loss = 0.0559, Val Loss = 0.1932\n","Epoch 145/200: Loss = 0.0558, Val Loss = 0.1948\n","Epoch 146/200: Loss = 0.0799, Val Loss = 0.1944\n","Epoch 147/200: Loss = 0.0455, Val Loss = 0.1946\n","Epoch 148/200: Loss = 0.0526, Val Loss = 0.1973\n","Epoch 149/200: Loss = 0.0365, Val Loss = 0.1999\n","Epoch 150/200: Loss = 0.0684, Val Loss = 0.1972\n","Epoch 151/200: Loss = 0.0559, Val Loss = 0.1992\n","Epoch 152/200: Loss = 0.0565, Val Loss = 0.2041\n","Epoch 153/200: Loss = 0.0495, Val Loss = 0.2006\n","Epoch 154/200: Loss = 0.0487, Val Loss = 0.1976\n","Epoch 155/200: Loss = 0.0438, Val Loss = 0.2014\n","Epoch 156/200: Loss = 0.0576, Val Loss = 0.2009\n","Epoch 157/200: Loss = 0.0386, Val Loss = 0.2041\n","Epoch 158/200: Loss = 0.0356, Val Loss = 0.2052\n","Epoch 159/200: Loss = 0.0521, Val Loss = 0.2050\n","Epoch 160/200: Loss = 0.0387, Val Loss = 0.2006\n","Epoch 161/200: Loss = 0.0348, Val Loss = 0.2057\n","Epoch 162/200: Loss = 0.0426, Val Loss = 0.2033\n","Epoch 163/200: Loss = 0.0320, Val Loss = 0.2042\n","Epoch 164/200: Loss = 0.0475, Val Loss = 0.2052\n","Epoch 165/200: Loss = 0.0234, Val Loss = 0.2055\n","Epoch 166/200: Loss = 0.0513, Val Loss = 0.2086\n","Epoch 167/200: Loss = 0.0406, Val Loss = 0.2069\n","Epoch 168/200: Loss = 0.0492, Val Loss = 0.2089\n","Epoch 169/200: Loss = 0.0617, Val Loss = 0.2097\n","Epoch 170/200: Loss = 0.0536, Val Loss = 0.2067\n","Epoch 171/200: Loss = 0.0226, Val Loss = 0.2159\n","Epoch 172/200: Loss = 0.0306, Val Loss = 0.2108\n","Epoch 173/200: Loss = 0.0510, Val Loss = 0.2087\n","Epoch 174/200: Loss = 0.0470, Val Loss = 0.2175\n","Epoch 175/200: Loss = 0.0257, Val Loss = 0.2091\n","Epoch 176/200: Loss = 0.0255, Val Loss = 0.2118\n","Epoch 177/200: Loss = 0.0406, Val Loss = 0.2105\n","Epoch 178/200: Loss = 0.0560, Val Loss = 0.2123\n","Epoch 179/200: Loss = 0.0299, Val Loss = 0.2165\n","Epoch 180/200: Loss = 0.0187, Val Loss = 0.2133\n","Epoch 181/200: Loss = 0.0245, Val Loss = 0.2149\n","Epoch 182/200: Loss = 0.0502, Val Loss = 0.2149\n","Epoch 183/200: Loss = 0.0342, Val Loss = 0.2143\n","Epoch 184/200: Loss = 0.0269, Val Loss = 0.2140\n","Epoch 185/200: Loss = 0.0229, Val Loss = 0.2205\n","Epoch 186/200: Loss = 0.0527, Val Loss = 0.2149\n","Epoch 187/200: Loss = 0.0212, Val Loss = 0.2148\n","Epoch 188/200: Loss = 0.0284, Val Loss = 0.2210\n","Epoch 189/200: Loss = 0.0536, Val Loss = 0.2238\n","Epoch 190/200: Loss = 0.0361, Val Loss = 0.2181\n","Epoch 191/200: Loss = 0.0488, Val Loss = 0.2215\n","Epoch 192/200: Loss = 0.0298, Val Loss = 0.2213\n","Epoch 193/200: Loss = 0.0689, Val Loss = 0.2250\n","Epoch 194/200: Loss = 0.0547, Val Loss = 0.2171\n","Epoch 195/200: Loss = 0.0264, Val Loss = 0.2220\n","Epoch 196/200: Loss = 0.0201, Val Loss = 0.2232\n","Epoch 197/200: Loss = 0.0262, Val Loss = 0.2199\n","Epoch 198/200: Loss = 0.0367, Val Loss = 0.2219\n","Epoch 199/200: Loss = 0.0482, Val Loss = 0.2233\n","Epoch 200/200: Loss = 0.0498, Val Loss = 0.2249\n"]}],"source":["# 初始化模型\n","model = MultiLabelCNN(vocab_size, embedding_dim, num_filters, filter_sizes, num_classes,dense_size)\n","model.load_pretrained_embeddings(weights)\n","model = model.to(device)\n","print(model)\n","# 训练模型\n","model.fit(X_train, y_train,x_val=X_valid, y_val=y_valid, num_epochs = num_epochs)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:22:14.321395Z","iopub.status.busy":"2023-05-22T07:22:14.320760Z","iopub.status.idle":"2023-05-22T07:22:14.332020Z","shell.execute_reply":"2023-05-22T07:22:14.329961Z","shell.execute_reply.started":"2023-05-22T07:22:14.321356Z"},"trusted":true},"outputs":[],"source":["y_test = test[[x for x in valid.columns[1:]]].values\n","y_test = torch.tensor(y_test).float()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:24:33.691953Z","iopub.status.busy":"2023-05-22T07:24:33.691578Z","iopub.status.idle":"2023-05-22T07:24:33.700686Z","shell.execute_reply":"2023-05-22T07:24:33.699593Z","shell.execute_reply.started":"2023-05-22T07:24:33.691915Z"},"trusted":true},"outputs":[],"source":["model.eval()  # 将模型切换到评估模式，禁用dropout\n","X_test = X_test.to(device)\n","with torch.no_grad():\n","    y_pred = model(X_test)\n","    y_pred = y_pred > 0.5"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:24:38.781538Z","iopub.status.busy":"2023-05-22T07:24:38.781169Z","iopub.status.idle":"2023-05-22T07:24:38.787392Z","shell.execute_reply":"2023-05-22T07:24:38.785835Z","shell.execute_reply.started":"2023-05-22T07:24:38.781508Z"},"trusted":true},"outputs":[],"source":["y_pred = y_pred.tolist()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T07:24:45.058742Z","iopub.status.busy":"2023-05-22T07:24:45.058375Z","iopub.status.idle":"2023-05-22T07:24:45.089448Z","shell.execute_reply":"2023-05-22T07:24:45.088499Z","shell.execute_reply.started":"2023-05-22T07:24:45.058715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.26      0.40        34\n","           1       0.89      0.67      0.76        81\n","           2       1.00      0.20      0.33         5\n","           3       0.84      0.80      0.82       317\n","           4       0.92      0.83      0.87        69\n","           5       1.00      0.62      0.76        34\n","           6       0.86      0.77      0.81       233\n","           7       0.87      0.72      0.79        46\n","           8       0.85      0.55      0.66       130\n","\n","   micro avg       0.86      0.71      0.78       949\n","   macro avg       0.89      0.60      0.69       949\n","weighted avg       0.86      0.71      0.77       949\n"," samples avg       0.85      0.78      0.79       949\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T08:34:11.066759Z","iopub.status.busy":"2023-05-11T08:34:11.066078Z","iopub.status.idle":"2023-05-11T08:34:11.077111Z","shell.execute_reply":"2023-05-11T08:34:11.075666Z","shell.execute_reply.started":"2023-05-11T08:34:11.066716Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[620   0]\n","  [ 34   0]]\n","\n"," [[573   0]\n","  [ 81   0]]\n","\n"," [[649   0]\n","  [  5   0]]\n","\n"," [[241  96]\n","  [ 97 220]]\n","\n"," [[585   0]\n","  [ 69   0]]\n","\n"," [[620   0]\n","  [ 34   0]]\n","\n"," [[421   0]\n","  [233   0]]\n","\n"," [[608   0]\n","  [ 46   0]]\n","\n"," [[524   0]\n","  [130   0]]]\n"]}],"source":["from sklearn.metrics import multilabel_confusion_matrix\n","confusion_matrix = multilabel_confusion_matrix(y_test, y_pred)\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","precision = precision_score(y_test, y_pred, average=None)\n","recall = recall_score(y_test, y_pred, average=None)\n","f1 = f1_score(y_test, y_pred, average=None)\n","\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'F1 Score: {f1}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":14154,"sourceId":19053,"sourceType":"datasetVersion"},{"datasetId":2579630,"sourceId":4397073,"sourceType":"datasetVersion"},{"datasetId":3086778,"sourceId":5629004,"sourceType":"datasetVersion"},{"datasetId":3257378,"sourceId":5666766,"sourceType":"datasetVersion"}],"dockerImageVersionId":30474,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
